{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection\n",
    "\n",
    "1. I selected the top 50 popular actors and actresses according to imdb https://www.imdb.com/list/ls053501318/. BeautifulSoup was used to scrap the webpage and retrieve their names\n",
    "2. The contents of the wikipages were downloaded using the wikipedia library in python. The contents were also saved into separate files.\n",
    "3. To get a baseline of prevalent words related to social movements, seven wikipages were downloaded: pages on 'LGBT social movements',Environmental movement','Human rights movement','Anti-war movement','Animal rights movement','Black Lives Matter','Civil rights movement'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.imdb.com/list/ls053501318/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "r  = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = r.text\n",
    "soup = BeautifulSoup(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting names of actors\n",
    "for element in soup.find_all('h3'):\n",
    "    try:\n",
    "        name = element.find('a').contents[0].strip() \n",
    "        if len(name)>0:\n",
    "            names.append(name)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Johnny Depp', 'Al Pacino', 'Robert De Niro', 'Kevin Spacey', 'Denzel Washington', 'Russell Crowe', 'Brad Pitt', 'Angelina Jolie', 'Leonardo DiCaprio', 'Tom Cruise']\n"
     ]
    }
   ],
   "source": [
    "print (names[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloading each person's wikipage using the wikipedia library\n",
    "for name in names:\n",
    "    wikipage = wikipedia.page(name)\n",
    "    if len(wikipage.content)>0:\n",
    "        fn = name.lower().replace(\" \",\"_\")\n",
    "        with open(f'{fn}.txt','w') as f: #saving each person's wikipage into separate files\n",
    "            f.write(wikipage.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "activist_pages = ['LGBT social movements','Environmental movement','Human rights movement',\n",
    "                 'Anti-war movement','Animal rights movement','Black Lives Matter',\n",
    "                 'Civil rights movement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "activist_txt = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in activist_pages:\n",
    "    activist_txt.append(wikipedia.page(p).content) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(activist_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleaning and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "#nltk.download('wordnet') \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stop_words.union(set(['people','child','parent','study','also','men','group','late'\n",
    "                                  'sometimes']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "\n",
    "    #Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    #remove tags\n",
    "    text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
    "\n",
    "    # remove special characters and digits\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "\n",
    "    ##Convert to list from string\n",
    "    text = text.split()\n",
    "\n",
    "    ##Stemming\n",
    "    ps=PorterStemmer()\n",
    "    #Lemmatisation\n",
    "    lem = WordNetLemmatizer()\n",
    "    text = [lem.lemmatize(word) for word in text if not word in  \n",
    "            stop_words] \n",
    "    text = \" \".join(text)\n",
    "    return (text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "activist_txt = [clean_text(i) for i in activist_txt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = []\n",
    "raw_paragraphs = []\n",
    "for name in names:\n",
    "    fn = name.lower().replace(\" \",\"_\")\n",
    "    with open(f'{fn}.txt','r') as f:\n",
    "        content = f.read().split(\"\\n\\n\\n\") ## break down wiki pages into smaller articles\n",
    "        raw_content = [i for i in content if len(i.split(\" \"))>10]\n",
    "        content = [clean_text(i) for i in content if len(i.split(\" \"))>10]\n",
    "        raw_paragraphs.extend(raw_content)\n",
    "        paragraphs.extend(content)\n",
    "#         content = f.read()\n",
    "#         paragraphs.append(clean_text(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "698"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting seed words for activists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most frequently occuring words\n",
    "def get_top_n_words(corpus, n=None):\n",
    "    vec = CountVectorizer(max_df=0.8,stop_words=stop_words, max_features=200, \n",
    "                    ngram_range=(1,3),min_df = 0.3).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in      \n",
    "                   vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
    "                       reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "#Convert most freq words to dataframe for plotting bar plot\n",
    "top_words = get_top_n_words(activist_txt, n=20)\n",
    "top_df = pd.DataFrame(top_words)\n",
    "top_df.columns=[\"Word\", \"Freq\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "activist_top_words = [i[0] for i in top_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['black',\n",
       " 'white',\n",
       " 'matter',\n",
       " 'civil right',\n",
       " 'police',\n",
       " 'human right',\n",
       " 'african american',\n",
       " 'law',\n",
       " 'king',\n",
       " 'march',\n",
       " 'community',\n",
       " 'freedom',\n",
       " 'city',\n",
       " 'local',\n",
       " 'officer',\n",
       " 'president',\n",
       " 'racial',\n",
       " 'day',\n",
       " 'civil right movement',\n",
       " 'liberation']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activist_top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guided LDA: using seed words to define topic, and retrieve similar documents in the topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import guidedlda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(max_df=0.8,stop_words=stop_words, \n",
    "                   ngram_range=(1,3),min_df = 0.01)\n",
    "X = np.array(vec.fit_transform(paragraphs).todense())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = vec.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tuple(word2id.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_topic_list = [activist_top_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = guidedlda.GuidedLDA(n_topics=5, n_iter=100, random_state=7, refresh=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:guidedlda:n_documents: 698\n",
      "INFO:guidedlda:vocab_size: 3464\n",
      "INFO:guidedlda:n_words: 109849\n",
      "INFO:guidedlda:n_topics: 5\n",
      "INFO:guidedlda:n_iter: 100\n",
      "INFO:guidedlda:<0> log likelihood: -1072245\n",
      "INFO:guidedlda:<20> log likelihood: -869446\n",
      "INFO:guidedlda:<40> log likelihood: -856846\n",
      "INFO:guidedlda:<60> log likelihood: -851806\n",
      "INFO:guidedlda:<80> log likelihood: -849698\n",
      "INFO:guidedlda:<99> log likelihood: -847731\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<guidedlda.guidedlda.GuidedLDA at 0x1a1ffc5780>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_topics = {}\n",
    "for t_id, st in enumerate(seed_topic_list):\n",
    "    for word in st:\n",
    "        if word in word2id.keys():\n",
    "#             print(word)\n",
    "            seed_topics[word2id[word]] = t_id\n",
    "model.fit(X, seed_topics=seed_topics, seed_confidence=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([199, 387, 528, 198, 258, 266, 264, 642, 262, 499])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doc_topic_[:,0].argsort()[::-1][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of paragraphs (articles) that are related to social/political progressive issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"=== 2016 Presidential election ===\\nFor the 2016 Republican Party presidential primaries, Schwarzenegger endorsed fellow Republican John Kasich. However, he announced in October that he would not vote for the Republican presidential candidate Donald Trump in that year's United States presidential election, with this being the first time he did not vote for the Republican candidate since becoming a citizen in 1983.\",\n",
       " '=== Political opinions ===\\nConnery is a member of the Scottish National Party (SNP), a centre-left political party campaigning for Scottish independence from the United Kingdom, and has supported the party financially and through personal appearances. His funding of the SNP ceased in 2001, when the UK Parliament passed legislation that prohibited overseas funding of political activities in the UK.',\n",
       " '=== Religious views ===\\nDuring a 1992 Vanity Fair interview, Nicholson stated, \"I don\\'t believe in God now. I can still work up an envy for someone who has faith. I can see how that could be a deeply soothing experience.\"',\n",
       " '=== Global warming ===\\nAt a 2015 security conference, Arnold Schwarzenegger called climate change the issue of our time.',\n",
       " '=== Beekeeping ===\\nAfter becoming concerned with the decline of honeybees, Freeman decided to turn his 124-acre ranch into a sanctuary for them in July 2014, starting with 26 bee hives.',\n",
       " '=== Politics ===\\nFreeman endorsed Barack Obama\\'s candidacy for the 2008 presidential election, although he stated that he would not join Obama\\'s campaign. He narrated for The Hall of Presidents with Obama, when he was added to the exhibit. The Hall of Presidents re-opened on July 4, 2009, at Walt Disney World Resort in Orlando, Florida. Freeman joined President Bill Clinton, USA Bid Committee Chairman Sunil Gulati, and USMNT midfielder Landon Donovan on December 1, 2010, in Zurich for the U.S. bid committee\\'s final presentation to FIFA for the 2022 FIFA World Cup. On day four of the 2016 Democratic National Convention, Morgan Freeman provided the voiceover for the video introduction of Democratic Presidential candidate Hillary Clinton.On September 19, 2017, Freeman featured in a video by the Committee to Investigate Russia group. In the video, Freeman declared \"we are at war\" with Russia. In April 2018, Freeman met with Saudi Arabia\\'s Crown Prince Mohammad bin Salman.',\n",
       " '=== Charitable work ===\\nIn 2004, Freeman and others formed the Grenada Relief Fund to aid people affected by Hurricane Ivan on the island of Grenada. The fund has since become PLANIT NOW, an organization that seeks to provide preparedness resources for people living in areas afflicted by hurricanes and severe storms. Freeman has worked on narrating small clips for global organizations, such as One Earth, whose goals include raising awareness of environmental issues. He has narrated the clip \"Why Are We Here”, which can be viewed on One Earth\\'s website. Freeman has donated money to the Mississippi Horse Park in Starkville, Mississippi. The park is part of Mississippi State University and Freeman has several horses that he takes there.',\n",
       " \"== Personal life ==\\nRyan married actor Dennis Quaid on February 14, 1991. They have one child together, Jack Quaid, born April 24, 1992. She and Quaid announced their separation in June 2000, and their divorce became final on July 16, 2001.In January 2006, Ryan adopted a 14-month-old girl from China whom she named Daisy True. Since 2010, Ryan has been in a relationship with American singer-songwriter John Mellencamp. On November 8, 2018, she announced her engagement to Mellencamp.Ryan supports the Democratic Party, especially its environmental protection programs and initiatives. In 2003, she supported Wesley Clark's campaign for U.S. president. She supported John Kerry during the 2004 presidential election.\",\n",
       " '=== Religious views ===\\nIn a 2012 interview with TheWrap, Freeman was asked if he considered himself atheist or agnostic. He replied, \"It\\'s a hard question because as I said at the start, I think we invented God. So if I believe in God, and I do, it\\'s because I think I\\'m God.\" Freeman later said that his experience working on The Story of God with Morgan Freeman did not change his views on religion.',\n",
       " \"=== Political views ===\\nClooney supported both of Barack Obama's 2008 and 2012 presidential campaigns. He is a supporter of gay rights. In 2016, Clooney endorsed Hillary Clinton for the 2016 presidential election.\"]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[raw_paragraphs[i] for i in model.doc_topic_[:,0].argsort()[::-1][:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
